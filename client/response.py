"""
This module defines the schema layer for standardizing LLM responses.

It providing structures to handle both streaming and non-streaming outputs
consistently, ensuring that information like text chunks, token usage,
and errors are captured in a uniform way across different LLM providers.
"""

# Imports
from __future__ import annotations # This is so that we can use TokenUsage in its own __add__ method.
from dataclasses import dataclass
from enum import StrEnum

@dataclass
class TextDelta:
    """
    Represents a small increment of text generated by the LLM.

    Used primarily during streaming to capture each piece of the response 
    as it arrives.
    
    Attributes:
        content: The actual text string in this delta.
    """
    content: str

    def __str__(self) -> str: # So, now not needed to use TextDelta.content to print the content.
        return self.content
    #In Python, __str__ is a Magic Method (also called a Dunder method). 
    # It tells Python: "Whenever someone tries to print(object) or convert this object to a string using str(object), 
    # return this specific text("Hello, world!") instead of the default gibberish i.e. something like <response.TextDelta object at 0x7f81b0>"

class StreamEventType(StrEnum):
    """
    Categorizes the types of events that can occur during an LLM response stream.
    """
    TEXT_DELTA = "text_delta" # Tiny chunks of text arriving
    MESSAGE_COMPLETE = "message_complete" # The full response has arrived
    ERROR = "error" # An error occurred 

@dataclass
class TokenUsage:
    """
    Tracks the number of tokens consumed during an LLM interaction.

    This helps in monitoring costs and managing context window limits.

    Attributes:
        completion_tokens: Tokens generated by the model.
        prompt_tokens: Tokens sent to the model in the prompt.
        total_tokens: Sum of prompt and completion tokens.
        cached_tokens: Tokens retrieved from cache (if supported).
    """
    completion_tokens: int = 0
    prompt_tokens: int = 0
    total_tokens: int = 0
    cached_tokens: int = 0

    def __add__(self, other: TokenUsage) -> TokenUsage:
        """Combines two TokenUsage objects to calculate total consumption."""
        return TokenUsage(
            completion_tokens=self.completion_tokens + other.completion_tokens,
            prompt_tokens=self.prompt_tokens + other.prompt_tokens,
            total_tokens=self.total_tokens + other.total_tokens,
            cached_tokens=self.cached_tokens + other.cached_tokens,
        )


# Intuition behind making `StreamEvent`
# When interacting with an LLM, the responses can vary. 
# You might get a full response  at once (non-streaming) or chunks of text over time (streaming) as LLMs are auto-regressive. 
# Additionally, the response might contain just text, or it might include tool calls, or even an error. 
# StreamEvent acts as a unified schema or class definition for any event that comes from the model. 
# This makes it easier to process and manage different types of responses consistently.

@dataclass
class StreamEvent:
    """
    A unified event object representing any state change in an LLM response stream.

    By using a single class for all types of response outcomes, we simplify the
    downstream processing logic in the Agent and UI.

    Attributes:
        type: The category of the stream event.
        text_delta: The text content (if applicable).
        error: Error message (if applicable).
        finish_reason: Why the LLM stopped generating (e.g., "stop", "length").
        usage: Detailed token usage statistics for the interaction.
    """
    type: StreamEventType
    text_delta: TextDelta | None = None # None in cases like when there is a tool call and such.
    error: str | None = None
    finish_reason: str | None = None
    usage: TokenUsage | None = None

    @classmethod
    def stream_error(cls, error: str) -> StreamEvent:
        """Creates an error event for the stream."""
        return cls(
            type=StreamEventType.ERROR,
            error=error
        )

    @classmethod
    def stream_text(cls, content: str) -> StreamEvent:
        """Creates a text delta event for the stream."""
        return cls(
            type=StreamEventType.TEXT_DELTA,
            text_delta=TextDelta(content=content),
        )

    @classmethod
    def stream_message_complete(
        cls,
        finish_reason: str,
        usage: TokenUsage,
        text_delta: TextDelta | None = None
    ) -> StreamEvent:
        """Creates an event signaling the final completion of a message."""
        return cls(
            type=StreamEventType.MESSAGE_COMPLETE,
            finish_reason=finish_reason,
            usage=usage,
            text_delta=text_delta,
        )


"""
Example: ChatCompletion Response Structure from OpenRouter API : Non-Streaming Case

ChatCompletion(
    id='gen-1768474359-QXxuX0mtG19eIkiwTwhK',
    choices=[Choice(
        finish_reason='stop',
        index=0,
        logprobs=None,
        message=ChatCompletionMessage(
            content="Hello! ðŸ˜Š I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with anything you need! How about youâ€”how are you doing today? Anything on your mind or something I can assist with?",
            refusal=None,
            role='assistant',
            annotations=None,
            audio=None,
            function_call=None,
            tool_calls=None,
            reasoning=None
        ),
        native_finish_reason='stop'
    )],
    created=1768474359,
    model='mistralai/devstral-2512:free',
    object='chat.completion',
    service_tier=None,
    system_fingerprint=None,
    usage=CompletionUsage(
        completion_tokens=55,
        prompt_tokens=9,
        total_tokens=64,
        completion_tokens_details=CompletionTokensDetails(
            accepted_prediction_tokens=None,
            audio_tokens=None,
            reasoning_tokens=0,
            rejected_prediction_tokens=None
        ),
        prompt_tokens_details=PromptTokensDetails(
            audio_tokens=0,
            cached_tokens=0
        ),
        cost=0,
        is_byok=False,
        cost_details={'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}
    ),
    provider='Mistral'
)

Example for Streaming Case:

ChatCompletionChunk(
    id='gen-1768494229-3gVPLV0aSEmZHC35TbRj',
    choices=[Choice(
        delta=ChoiceDelta(
            content=' ðŸ˜‚',
            function_call=None,
            refusal=None,
            role='assistant',
            tool_calls=None
        ),
        finish_reason=None,
        index=0,
        logprobs=None,
        native_finish_reason=None
    )],
    created=1768494229,
    model='mistralai/devstral-2512:free',
    object='chat.completion.chunk',
    service_tier=None,
    system_fingerprint=None,
    usage=None,
    provider='Mistral'
)

Usage info is available at last chunk of the response.

ChatCompletionChunk(
    id='gen-1768494229-3gVPLV0aSEmZHC35TbRj',
    choices=[Choice(
        delta=ChoiceDelta(
            content='',
            function_call=None,
            refusal=None,
            role='assistant',
            tool_calls=None
        ),
        finish_reason=None,
        index=0,
        logprobs=None,
        native_finish_reason=None
    )],
    created=1768494229,
    model='mistralai/devstral-2512:free',
    object='chat.completion.chunk',
    service_tier=None,
    system_fingerprint=None,
    usage=CompletionUsage(
        completion_tokens=41,
        prompt_tokens=10,
        total_tokens=51,
        completion_tokens_details=CompletionTokensDetails(
            accepted_prediction_tokens=None,
            audio_tokens=None,
            reasoning_tokens=0,
            rejected_prediction_tokens=None
        ),
        prompt_tokens_details=PromptTokensDetails(
            audio_tokens=0,
            cached_tokens=0
        ),
        cost=0,
        is_byok=False,
        cost_details={'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}
    ),
    provider='Mistral'
)
"""
