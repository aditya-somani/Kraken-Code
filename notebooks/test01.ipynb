{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554c8460",
   "metadata": {},
   "source": [
    "## Setting up LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f4d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "from openai import AsyncOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# https://openrouter.ai/docs/quickstart#using-the-openai-sdk -> OpenRouter Docs\n",
    "# https://github.com/openai/openai-python?tab=readme-ov-file#async-usage -> OpenAI Python SDK Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "933f9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMClient:\n",
    "    def __init__(self) -> None:\n",
    "        self._client : AsyncOpenAI | None = None\n",
    "\n",
    "    # We are not coupling the model when we are creating the client. So that afterwards, We can choose different models for different messages instead of having a same model.\n",
    "    def get_client(self) -> AsyncOpenAI:\n",
    "        if self._client is None:\n",
    "            self._client = AsyncOpenAI(\n",
    "                api_key=os.getenv(\"OPENROUTER_API_KEY\", \"\"),\n",
    "                base_url=os.getenv(\"OPENROUTER_BASE_URL\", \"\"),\n",
    "            )\n",
    "        return self._client            \n",
    "\n",
    "    async def close(self) -> None:\n",
    "        if self._client:\n",
    "            await self._client.close()\n",
    "            self._client = None\n",
    "\n",
    "    async def chat_completion(\n",
    "        self,\n",
    "        messages: list[dict[str, Any]],\n",
    "        stream: bool = True,\n",
    "    ):\n",
    "        client = self.get_client()\n",
    "        kwargs = {\n",
    "            \"model\": \"mistralai/devstral-2512:free\", # Free model - Hard coding for now...\n",
    "            \"messages\": messages,\n",
    "            \"stream\": stream,\n",
    "        }\n",
    "        \n",
    "        if stream:\n",
    "            return await self._stream_response()\n",
    "        else:\n",
    "            return await self._non_stream_response(client, kwargs)\n",
    "\n",
    "    async def _stream_response(self):\n",
    "        pass\n",
    "\n",
    "    async def _non_stream_response(\n",
    "        self,\n",
    "        client: AsyncOpenAI,\n",
    "        kwargs: dict[str, Any],\n",
    "    ):\n",
    "        response = await client.chat.completions.create(**kwargs)\n",
    "        choices = response.choices[0]\n",
    "        message = choices.message\n",
    "        content = None\n",
    "        \n",
    "        if message:\n",
    "            content = message.content\n",
    "        return response\n",
    "\n",
    "llm_client = LLMClient()\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "]\n",
    "response = await llm_client.chat_completion(messages, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159e216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='gen-1768484339-EN1ZkFUGJ5WL0zxKvSwK', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! ðŸ˜Š I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with anything you need! How about youâ€”how are you doing today? Anything on your mind or something I can assist with?\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1768484339, model='mistralai/devstral-2512:free', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=55, prompt_tokens=9, total_tokens=64, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0), cost=0, is_byok=False, cost_details={'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, 'upstream_inference_completions_cost': 0}), provider='Mistral')\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f373dd",
   "metadata": {},
   "source": [
    "Output -\n",
    "ChatCompletion(id='gen-1768299571-4Fh72rFHC1w2z1OOmMix', \n",
    "choices=[Choice(finish_reason='stop', index=0, logprobs=None, \n",
    "message=ChatCompletionMessage(content=\"Hello! ï¿½  I'm just a virtual assistant, so I don't have feelings, \n",
    "but I'm here and ready to help you with anything you need! How about youâ€”how are you doing today? \n",
    "Anything on your mind or something I can assist with?\", \n",
    "refusal=None, role='assistant', \n",
    "annotations=None, audio=None help you with anything you need! How about youâ€”how are you doing today? \n",
    "Anything on your mind or something I can assist with?\", \n",
    "refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), \n",
    "native_finish_reason='stop')], created=1768299572, model='mistralai/devstral-2512:free', object='chat.completion', \n",
    "service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=55, prompt_tokens=9, total_tokens=64, \n",
    "completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, \n",
    "rejected_prediction_tokens=None), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0), cost=0, \n",
    "is_byok=False, cost_details={'upstream_inference_cost': 0, 'upstream_inference_prompt_cost': 0, \n",
    "'upstream_inference_completions_cost': 0}), provider='Mistral')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f2b0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, with yield\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self) -> None:\n",
    "        self._client : AsyncOpenAI | None = None\n",
    "\n",
    "    # We are not coupling the model when we are creating the client. So that afterwards, We can choose different models for different messages instead of having a same model.\n",
    "    def get_client(self) -> AsyncOpenAI:\n",
    "        if self._client is None:\n",
    "            self._client = AsyncOpenAI(\n",
    "                api_key=os.getenv(\"OPENROUTER_API_KEY\", \"\"),\n",
    "                base_url=os.getenv(\"OPENROUTER_BASE_URL\", \"\"),\n",
    "            )\n",
    "        return self._client            \n",
    "\n",
    "    async def close(self) -> None:\n",
    "        if self._client:\n",
    "            await self._client.close()\n",
    "            self._client = None\n",
    "\n",
    "    async def chat_completion(\n",
    "        self,\n",
    "        messages: list[dict[str, Any]],\n",
    "        stream: bool = True,\n",
    "    ):\n",
    "        client = self.get_client()\n",
    "        kwargs = {\n",
    "            \"model\": \"mistralai/devstral-2512:free\", # Free model - Hard coding for now...\n",
    "            \"messages\": messages,\n",
    "            \"stream\": stream,\n",
    "        }\n",
    "        \n",
    "        if stream:\n",
    "            yield self._stream_response()\n",
    "        else:\n",
    "            yield self._non_stream_response(client, kwargs)\n",
    "\n",
    "    async def _stream_response(self):\n",
    "        pass\n",
    "\n",
    "    async def _non_stream_response(\n",
    "        self,\n",
    "        client: AsyncOpenAI,\n",
    "        kwargs: dict[str, Any],\n",
    "    ):\n",
    "        response = await client.chat.completions.create(**kwargs)\n",
    "        choices = response.choices[0]\n",
    "        message = choices.message\n",
    "        content = None\n",
    "        \n",
    "        if message:\n",
    "            content = message.content\n",
    "        yield response\n",
    "\n",
    "llm_client = LLMClient()\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "]\n",
    "response = llm_client.chat_completion(messages, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef55c798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<class 'async_generator'>, <async_generator object LLMClient.chat_completion at 0x00000260862CFD30>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response), response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a168f3",
   "metadata": {},
   "source": [
    "> Therefore, its a generator object i.e. AsyncGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434e88c",
   "metadata": {},
   "source": [
    "Had a little doubt regarding yield, so understanding it with this short example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbda4e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start of function ---\n",
      "First Item\n"
     ]
    }
   ],
   "source": [
    "def my_generator():\n",
    "    print(\"--- Start of function ---\")\n",
    "    yield \"First Item\"\n",
    "    \n",
    "    print(\"--- Continuing function ---\")\n",
    "    yield \"Second Item\"\n",
    "    \n",
    "    print(\"--- End of function ---\")\n",
    "\n",
    "# Calling the function doesn't run it! It just creates the \"Generator\"\n",
    "gen = my_generator()\n",
    "\n",
    "print(next(gen)) # Runs until the first yield\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b082560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Continuing function ---\n",
      "Second Item\n"
     ]
    }
   ],
   "source": [
    "print(next(gen)) # Picks up EXACTLY where it left off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d92e2",
   "metadata": {},
   "source": [
    "`yield` is a Python keyword used to create a Generator. Unlike return, which sends a final value and terminates the function, yield sends a value back to the caller and pauses the function's execution, preserving its entire state (variables, position, etc.). When the caller asks for the next value, the function resumes exactly where it left off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaedb9b1",
   "metadata": {},
   "source": [
    "### Understanding response.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69caa127",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextDelta() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTextDelta\u001b[39;00m:\n\u001b[32m      2\u001b[39m     content: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m text_delta = \u001b[43mTextDelta\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello, world!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: TextDelta() takes no arguments"
     ]
    }
   ],
   "source": [
    "class TextDelta:\n",
    "    content: str\n",
    "\n",
    "text_delta = TextDelta(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a462702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TextDelta:\n",
    "    content: str\n",
    "\n",
    "text_delta = TextDelta(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e9a3e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextDelta(content='Hello, world!')\n"
     ]
    }
   ],
   "source": [
    "print(text_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6faab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TextDelta:\n",
    "    content: str\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.content\n",
    "\n",
    "text_delta = TextDelta(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "911733f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "print(text_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1bfa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
